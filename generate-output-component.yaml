name: Generate movielens dataset for bigquery
inputs:
- {name: project_id, type: String}
- {name: raw_data_path, type: String}
- {name: batch_size, type: Integer}
- {name: rank_k, type: Integer}
- {name: num_actions, type: Integer}
- {name: driver_steps, type: Integer}
- {name: bigquery_tmp_file, type: String}
- {name: bigquery_dataset_id, type: String}
- {name: bigquery_location, type: String}
- {name: bigquery_table_id, type: String}
- {name: feature_id, type: String}
outputs:
- {name: bigquery_dataset_id, type: String}
- {name: bigquery_location, type: String}
- {name: bigquery_table_id, type: String}
implementation:
  container:
    image: tensorflow/tensorflow:2.5.0
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'google-cloud-bigquery==2.20.0' 'tensorflow==2.5.0' 'Image' 'tf-agents==0.8.0'
      || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'google-cloud-bigquery==2.20.0' 'tensorflow==2.5.0' 'Image' 'tf-agents==0.8.0'
      --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def generate_movielens_dataset_for_bigquery(
          project_id,
          raw_data_path,
          batch_size,
          rank_k,
          num_actions,
          driver_steps,
          bigquery_tmp_file,
          bigquery_dataset_id,
          bigquery_location,
          bigquery_table_id,
          feature_id
      ):

        # pylint: disable=g-import-not-at-top
        import collections
        import json
        from typing import Any, Dict

        from google.cloud import bigquery

        from tf_agents import replay_buffers
        from tf_agents import trajectories
        from tf_agents.bandits.agents.examples.v2 import trainer
        from tf_agents.bandits.environments import movielens_py_environment
        from tf_agents.drivers import dynamic_step_driver
        from tf_agents.environments import tf_py_environment
        from tf_agents.policies import random_tf_policy

        def generate_simulation_data(
            raw_data_path,
            batch_size,
            rank_k,
            num_actions,
            driver_steps):

          # Create movielens simulation environment.
          env = movielens_py_environment.MovieLensPyEnvironment(
              raw_data_path,
              rank_k,
              batch_size,
              num_movies=num_actions,
              csv_delimiter="\t")
          environment = tf_py_environment.TFPyEnvironment(env)

          # Define random policy for collecting data.
          random_policy = random_tf_policy.RandomTFPolicy(
              action_spec=environment.action_spec(),
              time_step_spec=environment.time_step_spec())

          # Use replay buffer and observers to keep track of Trajectory data.
          data_spec = random_policy.trajectory_spec
          replay_buffer = trainer.get_replay_buffer(data_spec, environment.batch_size,
                                                    driver_steps)
          observers = [replay_buffer.add_batch]

          # Run driver to apply the random policy in the simulation environment.
          driver = dynamic_step_driver.DynamicStepDriver(
              env=environment,
              policy=random_policy,
              num_steps=driver_steps * environment.batch_size,
              observers=observers)
          driver.run()

          return replay_buffer

        def build_dict_from_trajectory(
            trajectory):

          trajectory_dict = {
              "step_type": trajectory.step_type.numpy().tolist(),
              "observation": [{
                  "observation_batch": batch
              } for batch in trajectory.observation.numpy().tolist()],
              "action": trajectory.action.numpy().tolist(),
              "policy_info": trajectory.policy_info,
              "next_step_type": trajectory.next_step_type.numpy().tolist(),
              "reward": trajectory.reward.numpy().tolist(),
              "discount": trajectory.discount.numpy().tolist(),
          }
          return trajectory_dict

        def write_replay_buffer_to_file(
            replay_buffer,
            batch_size,
            dataset_file):

          dataset = replay_buffer.as_dataset(sample_batch_size=batch_size)
          dataset_size = replay_buffer.num_frames().numpy()

          with open(dataset_file, "w") as f:
            for example in dataset.take(count=dataset_size):
              traj_dict = build_dict_from_trajectory(example[0])
              f.write(json.dumps(traj_dict) + "\n")

        def load_dataset_into_bigquery(
            project_id,
            dataset_file,
            bigquery_dataset_id,
            bigquery_location,
            bigquery_table_id):

          # Construct a BigQuery client object.
          client = bigquery.Client(project=project_id)

          # Construct a full Dataset object to send to the API.
          dataset = bigquery.Dataset(bigquery_dataset_id)

          # Specify the geographic location where the dataset should reside.
          dataset.location = bigquery_location

          # Create the dataset, or get the dataset if it exists.
          dataset = client.create_dataset(dataset, exists_ok=True, timeout=30)

          job_config = bigquery.LoadJobConfig(
              schema=[
                  bigquery.SchemaField("step_type", "INT64", mode="REPEATED"),
                  bigquery.SchemaField(
                      "observation",
                      "RECORD",
                      mode="REPEATED",
                      fields=[
                          bigquery.SchemaField("observation_batch", "FLOAT64",
                                               "REPEATED")
                      ]),
                  bigquery.SchemaField("action", "INT64", mode="REPEATED"),
                  bigquery.SchemaField("policy_info", "FLOAT64", mode="REPEATED"),
                  bigquery.SchemaField("next_step_type", "INT64", mode="REPEATED"),
                  bigquery.SchemaField("reward", "FLOAT64", mode="REPEATED"),
                  bigquery.SchemaField("discount", "FLOAT64", mode="REPEATED"),
              ],
              source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
          )

          with open(dataset_file, "rb") as source_file:
            load_job = client.load_table_from_file(
                source_file, bigquery_table_id, job_config=job_config)

          load_job.result()  # Wait for the job to complete.

        replay_buffer = generate_simulation_data(
            raw_data_path=raw_data_path,
            batch_size=batch_size,
            rank_k=rank_k,
            num_actions=num_actions,
            driver_steps=driver_steps)

        write_replay_buffer_to_file(
            replay_buffer=replay_buffer,
            batch_size=batch_size,
            dataset_file=bigquery_tmp_file)

        load_dataset_into_bigquery(project_id, bigquery_tmp_file, bigquery_dataset_id,
                                   bigquery_location, bigquery_table_id)

        outputs = collections.namedtuple(
            "Outputs",
            ["bigquery_dataset_id", "bigquery_location", "bigquery_table_id"])

        return outputs(bigquery_dataset_id, bigquery_location, bigquery_table_id)

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                  str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Generate movielens dataset for bigquery', description='')
      _parser.add_argument("--project-id", dest="project_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--raw-data-path", dest="raw_data_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--batch-size", dest="batch_size", type=int, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--rank-k", dest="rank_k", type=int, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--num-actions", dest="num_actions", type=int, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--driver-steps", dest="driver_steps", type=int, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--bigquery-tmp-file", dest="bigquery_tmp_file", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--bigquery-dataset-id", dest="bigquery_dataset_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--bigquery-location", dest="bigquery_location", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--bigquery-table-id", dest="bigquery_table_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--feature-id", dest="feature_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = generate_movielens_dataset_for_bigquery(**_parsed_args)

      _output_serializers = [
          _serialize_str,
          _serialize_str,
          _serialize_str,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --project-id
    - {inputValue: project_id}
    - --raw-data-path
    - {inputValue: raw_data_path}
    - --batch-size
    - {inputValue: batch_size}
    - --rank-k
    - {inputValue: rank_k}
    - --num-actions
    - {inputValue: num_actions}
    - --driver-steps
    - {inputValue: driver_steps}
    - --bigquery-tmp-file
    - {inputValue: bigquery_tmp_file}
    - --bigquery-dataset-id
    - {inputValue: bigquery_dataset_id}
    - --bigquery-location
    - {inputValue: bigquery_location}
    - --bigquery-table-id
    - {inputValue: bigquery_table_id}
    - --feature-id
    - {inputValue: feature_id}
    - '----output-paths'
    - {outputPath: bigquery_dataset_id}
    - {outputPath: bigquery_location}
    - {outputPath: bigquery_table_id}
