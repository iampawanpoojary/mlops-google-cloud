name: Training op
inputs:
- {name: training_artifacts_dir, type: String}
- {name: tfrecord_file, type: String}
- {name: num_epochs, type: Integer}
- {name: rank_k, type: Integer}
- {name: num_actions, type: Integer}
- {name: tikhonov_weight, type: Float}
- {name: agent_alpha, type: Float}
outputs:
- {name: training_artifacts_dir, type: OutputPath}
implementation:
  container:
    image: python:3.7
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'tensorflow==2.5.0' 'tf-agents==0.8.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
      -m pip install --quiet --no-warn-script-location 'tensorflow==2.5.0' 'tf-agents==0.8.0'
      --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def training_op(\n    training_artifacts_dir,\n    tfrecord_file,\n    num_epochs,\n\
      \    rank_k,\n    num_actions,\n    tikhonov_weight,\n    agent_alpha\n):\n\
      \  import collections\n  from typing import Dict, List, NamedTuple \n\n  import\
      \ tensorflow as tf\n\n  from tf_agents import agents\n  from tf_agents import\
      \ policies\n  from tf_agents import trajectories\n  from tf_agents.bandits.agents\
      \ import lin_ucb_agent\n  from tf_agents.policies import policy_saver\n  from\
      \ tf_agents.specs import tensor_spec\n\n  import logging\n\n  per_arm = False\
      \  \n\n  feature_description = {\n      \"step_type\": tf.io.FixedLenFeature((),\
      \ tf.string),\n      \"observation\": tf.io.FixedLenFeature((), tf.string),\n\
      \      \"action\": tf.io.FixedLenFeature((), tf.string),\n      \"policy_info\"\
      : tf.io.FixedLenFeature((), tf.string),\n      \"next_step_type\": tf.io.FixedLenFeature((),\
      \ tf.string),\n      \"reward\": tf.io.FixedLenFeature((), tf.string),\n   \
      \   \"discount\": tf.io.FixedLenFeature((), tf.string),\n  }\n\n  def _parse_record(raw_record):\n\
      \    return tf.io.parse_single_example(raw_record, feature_description)\n\n\
      \  def build_trajectory(\n      parsed_record,\n      policy_info):\n    return\
      \ trajectories.Trajectory(\n        step_type=tf.expand_dims(\n            tf.io.parse_tensor(parsed_record[\"\
      step_type\"], out_type=tf.int32),\n            axis=1),\n        observation=tf.expand_dims(\n\
      \            tf.io.parse_tensor(\n                parsed_record[\"observation\"\
      ], out_type=tf.float32),\n            axis=1),\n        action=tf.expand_dims(\n\
      \            tf.io.parse_tensor(parsed_record[\"action\"], out_type=tf.int32),\n\
      \            axis=1),\n        policy_info=policy_info,\n        next_step_type=tf.expand_dims(\n\
      \            tf.io.parse_tensor(\n                parsed_record[\"next_step_type\"\
      ], out_type=tf.int32),\n            axis=1),\n        reward=tf.expand_dims(\n\
      \            tf.io.parse_tensor(parsed_record[\"reward\"], out_type=tf.float32),\n\
      \            axis=1),\n        discount=tf.expand_dims(\n            tf.io.parse_tensor(parsed_record[\"\
      discount\"], out_type=tf.float32),\n            axis=1))\n\n  def train_policy_on_trajectory(\n\
      \      agent,\n      tfrecord_file,\n      num_epochs\n  ):\n    raw_dataset\
      \ = tf.data.TFRecordDataset([tfrecord_file])\n    parsed_dataset = raw_dataset.map(_parse_record)\n\
      \n    train_loss = collections.defaultdict(list)\n    for epoch in range(num_epochs):\n\
      \      for parsed_record in parsed_dataset:\n        trajectory = build_trajectory(parsed_record,\
      \ agent.policy.info_spec)\n        loss, _ = agent.train(trajectory)\n     \
      \   train_loss[f\"epoch{epoch + 1}\"].append(loss.numpy())\n\n    train_outputs\
      \ = collections.namedtuple(\n        \"TrainOutputs\",\n        [\"policy\"\
      , \"train_loss\"])\n    return train_outputs(agent.policy, train_loss)\n\n \
      \ def execute_training_and_save_policy(\n      training_artifacts_dir,\n   \
      \   tfrecord_file,\n      num_epochs,\n      rank_k,\n      num_actions,\n \
      \     tikhonov_weight,\n      agent_alpha):\n    # Define time step and action\
      \ specs for one batch.\n    time_step_spec = trajectories.TimeStep(\n      \
      \  step_type=tensor_spec.TensorSpec(\n            shape=(), dtype=tf.int32,\
      \ name=\"step_type\"),\n        reward=tensor_spec.TensorSpec(\n           \
      \ shape=(), dtype=tf.float32, name=\"reward\"),\n        discount=tensor_spec.BoundedTensorSpec(\n\
      \            shape=(), dtype=tf.float32, name=\"discount\", minimum=0.,\n  \
      \          maximum=1.),\n        observation=tensor_spec.TensorSpec(\n     \
      \       shape=(rank_k,), dtype=tf.float32,\n            name=\"observation\"\
      ))\n\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n\
      \        dtype=tf.int32,\n        name=\"action\",\n        minimum=0,\n   \
      \     maximum=num_actions - 1)\n\n    # Define RL agent/algorithm.\n    agent\
      \ = lin_ucb_agent.LinearUCBAgent(\n        time_step_spec=time_step_spec,\n\
      \        action_spec=action_spec,\n        tikhonov_weight=tikhonov_weight,\n\
      \        alpha=agent_alpha,\n        dtype=tf.float32,\n        accepts_per_arm_features=per_arm)\n\
      \    agent.initialize()\n    logging.info(\"TimeStep Spec (for each batch):\\\
      n%s\\n\", agent.time_step_spec)\n    logging.info(\"Action Spec (for each batch):\\\
      n%s\\n\", agent.action_spec)\n\n    # Perform off-policy training.\n    policy,\
      \ _ = train_policy_on_trajectory(\n        agent=agent,\n        tfrecord_file=tfrecord_file,\n\
      \        num_epochs=num_epochs)\n\n    # Save trained policy.\n    saver = policy_saver.PolicySaver(policy)\n\
      \    saver.save(\"gs://mlops-vertex-capgemini/artifacts\")\n\n  execute_training_and_save_policy(\n\
      \      training_artifacts_dir=training_artifacts_dir,\n      tfrecord_file=tfrecord_file,\n\
      \      num_epochs=num_epochs,\n      rank_k=rank_k,\n      num_actions=num_actions,\n\
      \      tikhonov_weight=tikhonov_weight,\n      agent_alpha=agent_alpha)\n\n\
      \  outputs = collections.namedtuple(\n      \"Outputs\",\n      [\"training_artifacts_dir\"\
      ])\n\n  return outputs(training_artifacts_dir)\n\nimport argparse\n_parser =\
      \ argparse.ArgumentParser(prog='Training op', description='')\n_parser.add_argument(\"\
      --training-artifacts-dir\", dest=\"training_artifacts_dir\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--tfrecord-file\", dest=\"\
      tfrecord_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --num-epochs\", dest=\"num_epochs\", type=int, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--rank-k\", dest=\"rank_k\", type=int, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-actions\", dest=\"\
      num_actions\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --tikhonov-weight\", dest=\"tikhonov_weight\", type=float, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--agent-alpha\", dest=\"agent_alpha\", type=float, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
      _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = training_op(**_parsed_args)\n\
      \n_output_serializers = [\n    str,\n\n]\n\nimport os\nfor idx, output_file\
      \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --training-artifacts-dir
    - {inputValue: training_artifacts_dir}
    - --tfrecord-file
    - {inputValue: tfrecord_file}
    - --num-epochs
    - {inputValue: num_epochs}
    - --rank-k
    - {inputValue: rank_k}
    - --num-actions
    - {inputValue: num_actions}
    - --tikhonov-weight
    - {inputValue: tikhonov_weight}
    - --agent-alpha
    - {inputValue: agent_alpha}
    - '----output-paths'
    - {outputPath: training_artifacts_dir}
