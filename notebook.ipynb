{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLOps on Kubeflow and Feature store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO:\n",
    "- Bucket and pipeline and featurestore on different regions, recreate them on same regions based on featurestore avaliablity\n",
    "- Pipeline fails if import job already in progress, add code to wait till previous import is done\n",
    "- There was some errors with tf probablity on tensorflow 2.7 so had to downgrade to 2.5\n",
    "- Align on aiplatform sdk, currently both aiplatformv1 and aiplatformv1beta and kfp ai apltform is used\n",
    "- Have a good usecase to demo for featurestore serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %unload_ext nb_black\n",
    "# # magic cmd to autoformat code\n",
    "# #!pip install nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install {USER_FLAG} google-cloud-aiplatform==1.1.1\n",
    "# ! pip3 install {USER_FLAG} google-cloud-pipeline-components==0.1.3\n",
    "# ! pip3 install {USER_FLAG} --upgrade kfp\n",
    "# ! pip3 install {USER_FLAG} numpy==1.20.3\n",
    "# ! pip3 install --upgrade tensorflow\n",
    "# ! pip3 install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud.aiplatform_v1 import (\n",
    "    FeaturestoreOnlineServingServiceClient,\n",
    "    FeaturestoreServiceClient,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n",
    "from google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\n",
    "from google.cloud.aiplatform_v1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    featurestore_online_service as featurestore_online_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    featurestore_service as featurestore_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import io as io_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "from google.cloud import aiplatform\n",
    "from src.trainer import trainer_component\n",
    "from src.generator import generator_component\n",
    "from src.ingester import ingester_component\n",
    "from src import load_component\n",
    "import importlib\n",
    "from src import feature_store_helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIMBwJpVVaU3"
   },
   "source": [
    "#### Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "PhWoA2uCVaU3"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"mlop-cg-data-and-insights\"\n",
    "PROJECT_NO = \"389886591986\"\n",
    "BUCKET_NAME = \"gs://mlops-vertex-capgemini\"\n",
    "REGION = \"europe-west1\"\n",
    "\n",
    "API_ENDPOINT = \"europe-west3-aiplatform.googleapis.com\"\n",
    "INPUT_CSV_FILE = \"\"\n",
    "FEATURESTORE_ID = \"movie_prediction\"\n",
    "FEATURE_STORE_REGION = \"europe-west3\"\n",
    "ENTITY_TYPE_ID=\"movie_entity\"\n",
    "ENTITY_ID_FIELD=\"user_id\"\n",
    "\n",
    "# BigQuery parameters\n",
    "BIGQUERY_DATASET_ID = f\"{PROJECT_ID}.movielens_dataset\"\n",
    "BIGQUERY_LOCATION = \"EU\"\n",
    "BIGQUERY_TABLE_ID = f\"{BIGQUERY_DATASET_ID}.training_dataset\"\n",
    "BIGQUERY_RAW_TABLE_ID = f\"{BIGQUERY_DATASET_ID}.raw_dataset\"\n",
    "BIGQUERY_INPUT_URI=f\"bq://{BIGQUERY_RAW_TABLE_ID}\"\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/prajitdatta/movielens-100k-dataset\n",
    "# Dataset parameters\n",
    "RAW_DATA_PATH = \"gs://mlops-vertex-capgemini/dataset/u.data\"\n",
    "\n",
    "# u.data -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "# Each user has rated at least 20 movies. Users and items are\n",
    "# numbered consecutively from 1. The data is randomly\n",
    "# ordered. This is a tab separated list of\n",
    "# user id | item id | rating | timestamp.\n",
    "# The time stamps are unix seconds since 1/1/1970 UTC\n",
    "\n",
    "# Pipeline parameters\n",
    "PIPELINE_NAME = \"movie-prediction\"\n",
    "ENABLE_CACHING = False\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline\"\n",
    "PIPELINE_SPEC_PATH = \"metadata_pipeline.json\"\n",
    "OUTPUT_COMPONENT_SPEC = \"output-component.yaml\"\n",
    "BIGQUERY_TMP_FILE = \"tmp.json\"\n",
    "BIGQUERY_MAX_ROWS = 5\n",
    "TFRECORD_FILE = f\"{BUCKET_NAME}/trainer_input_path/tf\"\n",
    "LOGGER_PUBSUB_TOPIC = \"logger-pubsub-topic\"\n",
    "LOGGER_CLOUD_FUNCTION = \"logger-cloud-function\"\n",
    "\n",
    "# Trainer parameters\n",
    "TRAINING_ARTIFACTS_DIR = f\"{BUCKET_NAME}/artifacts\"\n",
    "TRAINING_REPLICA_COUNT = \"1\"\n",
    "TRAINING_MACHINE_TYPE = \"n1-standard-4\"\n",
    "TRAINING_ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "TRAINING_ACCELERATOR_COUNT = \"0\"\n",
    "\n",
    "# Deployer parameters\n",
    "TRAINED_POLICY_DISPLAY_NAME = \"movielens-trained-policy\"\n",
    "ENDPOINT_DISPLAY_NAME = \"movielens-endpoint\"\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Prediction container parameters\n",
    "PREDICTION_CONTAINER = \"prediction-container\"\n",
    "PREDICTION_CONTAINER_DIR = \"src/prediction_container\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1979226  2021-12-14T10:28:10Z  gs://mlops-vertex-capgemini/u.data#1639477690746113  metageneration=1\n",
      "   1979259  2021-12-14T11:20:28Z  gs://mlops-vertex-capgemini/uasdf.csv#1639480828395119  metageneration=1\n",
      "                                 gs://mlops-vertex-capgemini/artifacts/\n",
      "                                 gs://mlops-vertex-capgemini/dataset/\n",
      "                                 gs://mlops-vertex-capgemini/europe-west1-projects/\n",
      "                                 gs://mlops-vertex-capgemini/pipeline/\n",
      "                                 gs://mlops-vertex-capgemini/trainer_input_path/\n",
      "TOTAL: 2 objects, 3958485 bytes (3.78 MiB)\n"
     ]
    }
   ],
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_NAME\n",
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long running operation: projects/389886591986/locations/europe-west3/featurestores/movie_prediction/operations/4115397255974879232\n",
      "create_featurestore_response: name: \"projects/389886591986/locations/europe-west3/featurestores/movie_prediction\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_helper.create_featurestore(\n",
    "    project=PROJECT_ID,\n",
    "    featurestore_id=FEATURESTORE_ID,\n",
    "    location=FEATURE_STORE_REGION,\n",
    "    api_endpoint=API_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Featurestores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurestores found: 1\n",
      "['projects/389886591986/locations/europe-west3/featurestores/movie_prediction']\n"
     ]
    }
   ],
   "source": [
    "# get featurestore list to verify, can use this code later to cleanup\n",
    "featurestore_list = feature_store_helper.list_featurestore(\n",
    "    project=PROJECT_ID, location=FEATURE_STORE_REGION\n",
    ")\n",
    "print(featurestore_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long running operation: projects/389886591986/locations/europe-west3/featurestores/movie_prediction/entityTypes/movie_entity/operations/9006306451299237888\n",
      "create_entity_type_response: name: \"projects/389886591986/locations/europe-west3/featurestores/movie_prediction/entityTypes/movie_entity\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_helper.create_entity_type(project=PROJECT_ID,\n",
    "                                        featurestore_id=FEATURESTORE_ID,\n",
    "                                        entity_type_id=ENTITY_TYPE_ID,\n",
    "                                        description=\"movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long running operation: projects/389886591986/locations/europe-west3/featurestores/movie_prediction/operations/620603945135374336\n",
      "create_feature_response: name: \"projects/389886591986/locations/europe-west3/featurestores/movie_prediction/entityTypes/movie_entity/features/user_id\"\n",
      "\n",
      "Long running operation: projects/389886591986/locations/europe-west3/featurestores/movie_prediction/operations/3669540892865200128\n",
      "create_feature_response: name: \"projects/389886591986/locations/europe-west3/featurestores/movie_prediction/entityTypes/movie_entity/features/item_id\"\n",
      "\n",
      "Long running operation: projects/389886591986/locations/europe-west3/featurestores/movie_prediction/operations/6425743864815943680\n",
      "create_feature_response: name: \"projects/389886591986/locations/europe-west3/featurestores/movie_prediction/entityTypes/movie_entity/features/rating\"\n",
      "\n",
      "Long running operation: projects/389886591986/locations/europe-west3/featurestores/movie_prediction/operations/2165338617323454464\n",
      "create_feature_response: name: \"projects/389886591986/locations/europe-west3/featurestores/movie_prediction/entityTypes/movie_entity/features/timestamp\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_helper.create_feature(project=PROJECT_ID,\n",
    "                                    featurestore_id=FEATURESTORE_ID,\n",
    "                                    entity_type_id=ENTITY_TYPE_ID,\n",
    "                                    feature_id=\"user_id\",\n",
    "                                    value_type=aiplatform.gapic.Feature.ValueType.STRING)\n",
    "feature_store_helper.create_feature(project=PROJECT_ID,\n",
    "                                    featurestore_id=FEATURESTORE_ID,\n",
    "                                    entity_type_id=ENTITY_TYPE_ID,\n",
    "                                    feature_id=\"item_id\",\n",
    "                                    value_type=aiplatform.gapic.Feature.ValueType.STRING)\n",
    "feature_store_helper.create_feature(project=PROJECT_ID,\n",
    "                                    featurestore_id=FEATURESTORE_ID,\n",
    "                                    entity_type_id=ENTITY_TYPE_ID,\n",
    "                                    feature_id=\"rating\",\n",
    "                                    value_type=aiplatform.gapic.Feature.ValueType.STRING)\n",
    "feature_store_helper.create_feature(project=PROJECT_ID,\n",
    "                                    featurestore_id=FEATURESTORE_ID,\n",
    "                                    entity_type_id=ENTITY_TYPE_ID,\n",
    "                                    feature_id=\"timestamp\",\n",
    "                                    value_type=aiplatform.gapic.Feature.ValueType.STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup and delete featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_store_helper.cleanup_featurestore(PROJECT_ID, FEATURESTORE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloudbuild file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUvEWUucibYc"
   },
   "outputs": [],
   "source": [
    "# cloudbuild_yaml = \"\"\"steps:\n",
    "# - name: \"gcr.io/kaniko-project/executor:latest\"\n",
    "#   args: [\"--destination=gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\",\n",
    "#          \"--cache=true\",\n",
    "#          \"--cache-ttl=99h\"]\n",
    "#   env: [\"AIP_STORAGE_URI={ARTIFACTS_DIR}\",\n",
    "#         \"PROJECT_ID={PROJECT_ID}\",\n",
    "#         \"LOGGER_PUBSUB_TOPIC={LOGGER_PUBSUB_TOPIC}\"]\n",
    "# options:\n",
    "#   machineType: \"E2_HIGHCPU_8\"\n",
    "# \"\"\".format(\n",
    "#     PROJECT_ID=PROJECT_ID,\n",
    "#     PREDICTION_CONTAINER=PREDICTION_CONTAINER,\n",
    "#     ARTIFACTS_DIR=TRAINING_ARTIFACTS_DIR,\n",
    "#     LOGGER_PUBSUB_TOPIC=LOGGER_PUBSUB_TOPIC,\n",
    "# )\n",
    "\n",
    "# with open(f\"{PREDICTION_CONTAINER_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "#     fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uHbODeXibYd"
   },
   "outputs": [],
   "source": [
    "# ! gcloud builds submit --config $PREDICTION_CONTAINER_DIR/cloudbuild.yaml $PREDICTION_CONTAINER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(load_component)\n",
    "load_component.load_raw_data_to_bigquery(PROJECT_ID,RAW_DATA_PATH,BIGQUERY_DATASET_ID, BIGQUERY_LOCATION, BIGQUERY_TABLE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(generator_component)\n",
    "generator_component.generate_movielens_dataset_for_bigquery(PROJECT_ID,RAW_DATA_PATH,8,20,20,3, BIGQUERY_TMP_FILE, BIGQUERY_DATASET_ID, BIGQUERY_LOCATION,BIGQUERY_TABLE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ingester_component)\n",
    "ingester_component.ingest_bigquery_dataset_into_tfrecord(PROJECT_ID, BIGQUERY_TABLE_ID,TFRECORD_FILE, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for manually verifying your tfrecord dataset\n",
    "import tensorflow as tf \n",
    "raw_dataset = tf.data.TFRecordDataset(\"tf\")\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(trainer_component)\n",
    "trainer_component.training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_helper.import_feature_values(project=PROJECT_ID,\n",
    "                                    featurestore_id=FEATURESTORE_ID,\n",
    "                                    entity_type_id=ENTITY_TYPE_ID,\n",
    "                                    bigquery_uri=bigquery_source,\n",
    "                                    entity_id_field=ENTITY_ID_FIELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.bigquery_to_featurestore' from '/home/jupyter/pawan/mlops/notebook/pipeline/src/bigquery_to_featurestore.py'>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import bigquery_to_featurestore\n",
    "importlib.reload(bigquery_to_featurestore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long running operation: projects/389886591986/locations/europe-west3/featurestores/movie_prediction/entityTypes/movie_entity/operations/2241899810988752896\n",
      "import_feature_values_response: imported_entity_count: 100000\n",
      "imported_feature_value_count: 400000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'movie_prediction'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigquery_to_featurestore.import_feature_values(project=PROJECT_ID,\n",
    "                                    featurestore_id=FEATURESTORE_ID,\n",
    "                                    entity_type_id=ENTITY_TYPE_ID,\n",
    "                                    bigquery_uri=bigquery_source,\n",
    "                                    entity_id_field=ENTITY_ID_FIELD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "cvXJzhSSGilq"
   },
   "outputs": [],
   "source": [
    "from kfp.components import load_component_from_url\n",
    "from kfp.components import create_component_from_func\n",
    "\n",
    "from src.trainer import trainer_component\n",
    "from src.generator import generator_component\n",
    "from src.ingester import ingester_component\n",
    "from src import load_component\n",
    "from src import bigquery_to_featurestore\n",
    "\n",
    "\n",
    "# so jupyter kernel reloads the modules when we change them\n",
    "importlib.reload(load_component)\n",
    "importlib.reload(generator_component)\n",
    "importlib.reload(ingester_component)\n",
    "importlib.reload(bigquery_to_featurestore)\n",
    "\n",
    "\n",
    "@dsl.pipeline(pipeline_root=PIPELINE_ROOT, name=f\"{PIPELINE_NAME}-startup\")\n",
    "def pipeline(\n",
    "    # Pipeline configs\n",
    "    project_id: str,\n",
    "    raw_data_path: str,\n",
    "    training_artifacts_dir: str,\n",
    "    featurestore_id: str,\n",
    "    entity_type_id: str,\n",
    "    bigquery_uri: str,\n",
    "    entity_id_field: str,\n",
    "    # BigQuery configs\n",
    "    bigquery_dataset_id: str,\n",
    "    bigquery_location: str,\n",
    "    bigquery_table_id: str,\n",
    "    bigquery_raw_table_id: str,\n",
    "    bigquery_max_rows: int = 10000,\n",
    "    # TF-Agents RL configs\n",
    "    batch_size: int = 8,\n",
    "    rank_k: int = 20,\n",
    "    num_actions: int = 20,\n",
    "    driver_steps: int = 3,\n",
    "    num_epochs: int = 5,\n",
    "    tikhonov_weight: float = 0.01,\n",
    "    agent_alpha: float = 10,\n",
    ") -> None:\n",
    "    \n",
    "    load_op = create_component_from_func(\n",
    "    func=load_component.load_raw_data_to_bigquery,\n",
    "    output_component_file=\"load-output-component.yaml\",\n",
    "    packages_to_install=[\n",
    "      \"google-cloud-bigquery==2.20.0\",\n",
    "    ],\n",
    "  )\n",
    "    load_task = load_op(\n",
    "        project_id=project_id,\n",
    "        raw_data_path=raw_data_path,\n",
    "        bigquery_dataset_id=bigquery_dataset_id,\n",
    "        bigquery_location=bigquery_location,\n",
    "        bigquery_table_id=bigquery_raw_table_id,\n",
    "    )\n",
    "    \n",
    "    preprocess_op = create_component_from_func(\n",
    "    func=bigquery_to_featurestore.import_feature_values,\n",
    "    output_component_file=\"preprocess-output-component.yaml\",\n",
    "    packages_to_install=[\n",
    "      \"google-cloud-aiplatform\",\n",
    "    ],\n",
    "    )\n",
    "    preprocess_task = preprocess_op(\n",
    "        project=project_id,\n",
    "        featurestore_id=featurestore_id,\n",
    "        entity_type_id=entity_type_id,\n",
    "        bigquery_uri=bigquery_uri,\n",
    "        bigquery_table_id=load_task.outputs[\"bigquery_table_id\"],\n",
    "        entity_id_field=entity_id_field, \n",
    "    )\n",
    "    \n",
    "    generate_op = create_component_from_func(\n",
    "    func=generator_component.generate_movielens_dataset_for_bigquery,\n",
    "    base_image=\"tensorflow/tensorflow:2.5.0\",\n",
    "    output_component_file=\"generate-output-component.yaml\",\n",
    "    packages_to_install=[\n",
    "      \"google-cloud-bigquery==2.20.0\",\n",
    "      \"tensorflow==2.5.0\",\n",
    "      \"Image\",\n",
    "      \"tf-agents==0.8.0\",\n",
    "    ],\n",
    "  )\n",
    "\n",
    "    # Run the Generator component.\n",
    "    generate_task = generate_op(\n",
    "        project_id=project_id,\n",
    "        raw_data_path=raw_data_path,\n",
    "        batch_size=batch_size,\n",
    "        rank_k=rank_k,\n",
    "        num_actions=num_actions,\n",
    "        driver_steps=driver_steps,\n",
    "        bigquery_tmp_file=BIGQUERY_TMP_FILE,\n",
    "        bigquery_dataset_id=bigquery_dataset_id,\n",
    "        bigquery_location=bigquery_location,\n",
    "        bigquery_table_id=bigquery_table_id,\n",
    "        feature_id=preprocess_task.outputs[\"featurestore_id\"],\n",
    "    )\n",
    "    \n",
    "    ingest_op = create_component_from_func(\n",
    "    func=ingester_component.ingest_bigquery_dataset_into_tfrecord,\n",
    "    base_image=\"tensorflow/tensorflow:2.5.0\",\n",
    "    output_component_file=f\"ingest-{OUTPUT_COMPONENT_SPEC}\",\n",
    "    packages_to_install=[\n",
    "      \"google-cloud-bigquery==2.20.0\",\n",
    "      \"tensorflow==2.5.0\",\n",
    "    ],\n",
    "  )\n",
    "\n",
    "    # Run the Ingester component.\n",
    "    ingest_task = ingest_op(\n",
    "        project_id=project_id,\n",
    "        bigquery_table_id=generate_task.outputs[\"bigquery_table_id\"],\n",
    "        bigquery_max_rows=bigquery_max_rows,\n",
    "        tfrecord_file=TFRECORD_FILE,\n",
    "    )\n",
    "\n",
    "    # Run the Trainer component and submit custom job to Vertex AI.\n",
    "    train_op = create_component_from_func(\n",
    "      func=trainer_component.training_op,\n",
    "      output_component_file=f\"trainer-{OUTPUT_COMPONENT_SPEC}\",\n",
    "      packages_to_install=[\n",
    "          \"tensorflow==2.5.0\",\n",
    "          \"tf-agents==0.8.0\",\n",
    "      ])\n",
    "\n",
    "    train_task = train_op(\n",
    "      training_artifacts_dir=training_artifacts_dir,\n",
    "      # tfrecord_file=ingest_task.outputs[\"tfrecord_file\"],\n",
    "      tfrecord_file=\"gs://mlops-vertex-capgemini/trainer_input_path/tf\",\n",
    "      num_epochs=num_epochs,\n",
    "      rank_k=rank_k,\n",
    "      num_actions=num_actions,\n",
    "      tikhonov_weight=tikhonov_weight,\n",
    "      agent_alpha=agent_alpha)\n",
    "\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": train_task.container.image,\n",
    "            },\n",
    "            \"replicaCount\": TRAINING_REPLICA_COUNT,\n",
    "            \"machineSpec\": {\n",
    "                \"machineType\": TRAINING_MACHINE_TYPE,\n",
    "                \"acceleratorType\": TRAINING_ACCELERATOR_TYPE,\n",
    "                \"acceleratorCount\": TRAINING_ACCELERATOR_COUNT,\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "    train_task.custom_job_spec = {\n",
    "        \"displayName\": train_task.name,\n",
    "        \"jobSpec\": {\n",
    "            \"workerPoolSpecs\": worker_pool_specs,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # # Run the Deployer components.\n",
    "    # # Upload the trained policy as a model.\n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "      project=project_id,\n",
    "      location=REGION,\n",
    "      display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "      artifact_uri=training_artifacts_dir,\n",
    "      serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\",\n",
    "    )\n",
    "    train_task.after(ingest_task)\n",
    "    # # Model uploading has to occur after training completes.\n",
    "    model_upload_op.after(train_task)\n",
    "    # # Create a Vertex AI endpoint. (This operation can occur in parallel with\n",
    "    # # the Generator, Ingester, Trainer components.)\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "      project=project_id,\n",
    "      location=REGION,\n",
    "      display_name=ENDPOINT_DISPLAY_NAME)\n",
    "    # Deploy the uploaded, trained policy to the created endpoint. (This operation\n",
    "    # has to occur after both model uploading and endpoint creation complete.)\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "      project=project_id,\n",
    "      endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "      model=model_upload_op.outputs[\"model\"],\n",
    "      #endpoint=\"495264017615421440\",\n",
    "      #model=\"944845526120005632\",\n",
    "      deployed_model_display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "      machine_type=ENDPOINT_MACHINE_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "icYK0WoRGilr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/movie-prediction-startup-20211215125434?project=mlop-cg-data-and-insights\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the authored pipeline.\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=PIPELINE_SPEC_PATH)\n",
    "\n",
    "# Createa Vertex AI client.\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n",
    "\n",
    "# Create a pipeline run job.\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=PIPELINE_SPEC_PATH,\n",
    "    parameter_values={\n",
    "        # Pipeline configs\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"raw_data_path\": RAW_DATA_PATH,\n",
    "        \"training_artifacts_dir\": TRAINING_ARTIFACTS_DIR,\n",
    "        \"featurestore_id\": FEATURESTORE_ID,\n",
    "        \"entity_type_id\": ENTITY_TYPE_ID,\n",
    "        \"bigquery_uri\": BIGQUERY_INPUT_URI,\n",
    "        \"entity_id_field\": ENTITY_ID_FIELD,\n",
    "        # BigQuery configs\n",
    "        \"bigquery_dataset_id\": BIGQUERY_DATASET_ID,\n",
    "        \"bigquery_location\": BIGQUERY_LOCATION,\n",
    "        \"bigquery_table_id\": BIGQUERY_TABLE_ID,\n",
    "        \"bigquery_raw_table_id\": BIGQUERY_RAW_TABLE_ID,\n",
    "    },\n",
    "    enable_caching=ENABLE_CACHING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run locally for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=PROJECT_ID\n",
    "raw_data_path= RAW_DATA_PATH\n",
    "training_artifacts_dir= TRAINING_ARTIFACTS_DIR\n",
    "# BigQuery configs\n",
    "bigquery_dataset_id= BIGQUERY_DATASET_ID\n",
    "bigquery_location= BIGQUERY_LOCATION\n",
    "bigquery_table_id= BIGQUERY_TABLE_ID\n",
    "tfrecord_file=\"gs://mlops-vertex-capgemini/trainer_input_path/tf\"\n",
    "bigquery_max_rows= 10000\n",
    "# TF-Agents RL configs\n",
    "batch_size=  8\n",
    "rank_k= 20\n",
    "num_actions= 20\n",
    "driver_steps= 3\n",
    "num_epochs= 5\n",
    "tikhonov_weight: float = 0.01\n",
    "agent_alpha: float = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start\")\n",
    "\"\"\"The Trainer component for training a policy on TFRecord files.\"\"\"\n",
    "# Import for the function return value type.\n",
    "from typing import NamedTuple  # pylint: disable=unused-import\n",
    "\n",
    "from kfp import components\n",
    "\n",
    "\n",
    "import collections\n",
    "from typing import Dict, List, NamedTuple  # pylint: disable=redefined-outer-name,reimported\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents import agents\n",
    "from tf_agents import policies\n",
    "from tf_agents import trajectories\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "import logging\n",
    "\n",
    "per_arm = False  # Using the non-per-arm version of the movie environment.\n",
    "\n",
    "# Mapping from feature name to serialized value\n",
    "feature_description = {\n",
    "    \"step_type\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"observation\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"action\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"policy_info\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"next_step_type\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"reward\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"discount\": tf.io.FixedLenFeature((), tf.string),\n",
    "}\n",
    "\n",
    "def _parse_record(raw_record: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "    \"\"\"Parses a serialized `tf.train.Example` proto.\n",
    "    Args:\n",
    "    raw_record: A serialized data record of a `tf.train.Example` proto.\n",
    "    Returns:\n",
    "    A dict mapping feature names to values as `tf.Tensor` objects of type\n",
    "    string containing serialized protos, following `feature_description`.\n",
    "    \"\"\"\n",
    "    return tf.io.parse_single_example(raw_record, feature_description)\n",
    "\n",
    "def build_trajectory(\n",
    "    parsed_record: Dict[str, tf.Tensor],\n",
    "    policy_info: policies.utils.PolicyInfo) -> trajectories.Trajectory:\n",
    "    \"\"\"Builds a `trajectories.Trajectory` object from `parsed_record`.\n",
    "    Args:\n",
    "    parsed_record: A dict mapping feature names to values as `tf.Tensor`\n",
    "        objects of type string containing serialized protos.\n",
    "    policy_info: Policy information specification.\n",
    "    Returns:\n",
    "    A `trajectories.Trajectory` object that contains values as de-serialized\n",
    "    `tf.Tensor` objects from `parsed_record`.\n",
    "    \"\"\"\n",
    "    return trajectories.Trajectory(\n",
    "        step_type=tf.expand_dims(\n",
    "            tf.io.parse_tensor(parsed_record[\"step_type\"], out_type=tf.int32),\n",
    "            axis=1),\n",
    "        observation=tf.expand_dims(\n",
    "            tf.io.parse_tensor(\n",
    "                parsed_record[\"observation\"], out_type=tf.float32),\n",
    "            axis=1),\n",
    "        action=tf.expand_dims(\n",
    "            tf.io.parse_tensor(parsed_record[\"action\"], out_type=tf.int32),\n",
    "            axis=1),\n",
    "        policy_info=policy_info,\n",
    "        next_step_type=tf.expand_dims(\n",
    "            tf.io.parse_tensor(\n",
    "                parsed_record[\"next_step_type\"], out_type=tf.int32),\n",
    "            axis=1),\n",
    "        reward=tf.expand_dims(\n",
    "            tf.io.parse_tensor(parsed_record[\"reward\"], out_type=tf.float32),\n",
    "            axis=1),\n",
    "        discount=tf.expand_dims(\n",
    "            tf.io.parse_tensor(parsed_record[\"discount\"], out_type=tf.float32),\n",
    "            axis=1))\n",
    "\n",
    "def train_policy_on_trajectory(\n",
    "    agent: agents.TFAgent,\n",
    "    tfrecord_file: str,\n",
    "    num_epochs: int\n",
    ") -> NamedTuple(\"TrainOutputs\", [\n",
    "    (\"policy\", policies.TFPolicy),\n",
    "    (\"train_loss\", Dict[str, List[float]]),\n",
    "]):\n",
    "    \"\"\"Trains the policy in `agent` on the dataset of `tfrecord_file`.\n",
    "    Parses `tfrecord_file` as `tf.train.Example` objects, packages them into\n",
    "    `trajectories.Trajectory` objects, and trains the agent's policy on these\n",
    "    trajectory objects.\n",
    "    Args:\n",
    "    agent: A TF-Agents agent that carries the policy to train.\n",
    "    tfrecord_file: Path to the TFRecord file containing the training dataset.\n",
    "    num_epochs: Number of epochs to train the policy.\n",
    "    Returns:\n",
    "    A NamedTuple of (a trained TF-Agents policy, a dict mapping from\n",
    "    \"epoch<i>\" to lists of loss values produced at each training step).\n",
    "    \"\"\"\n",
    "    raw_dataset = tf.data.TFRecordDataset([tfrecord_file])\n",
    "    parsed_dataset = raw_dataset.map(_parse_record)\n",
    "\n",
    "    train_loss = collections.defaultdict(list)\n",
    "    for epoch in range(num_epochs):\n",
    "        for parsed_record in parsed_dataset:\n",
    "            trajectory = build_trajectory(parsed_record, agent.policy.info_spec)\n",
    "            loss, _ = agent.train(trajectory)\n",
    "            train_loss[f\"epoch{epoch + 1}\"].append(loss.numpy())\n",
    "\n",
    "    train_outputs = collections.namedtuple(\n",
    "        \"TrainOutputs\",\n",
    "        [\"policy\", \"train_loss\"])\n",
    "    return train_outputs(agent.policy, train_loss)\n",
    "\n",
    "def execute_training_and_save_policy(\n",
    "    training_artifacts_dir: str,\n",
    "    tfrecord_file: str,\n",
    "    num_epochs: int,\n",
    "    rank_k: int,\n",
    "    num_actions: int,\n",
    "    tikhonov_weight: float,\n",
    "    agent_alpha: float) -> None:\n",
    "    \"\"\"Executes training for the policy and saves the policy.\n",
    "    Args:\n",
    "    training_artifacts_dir: Path to store the Trainer artifacts (trained\n",
    "        policy).\n",
    "    tfrecord_file: Path to file to write the ingestion result TFRecords.\n",
    "    num_epochs: Number of training epochs.\n",
    "    rank_k: Rank for matrix factorization in the movie environment; also\n",
    "        the observation dimension.\n",
    "    num_actions: Number of actions (movie items) to choose from.\n",
    "    tikhonov_weight: LinUCB Tikhonov regularization weight of the Trainer.\n",
    "    agent_alpha: LinUCB exploration parameter that multiplies the confidence\n",
    "        intervals of the Trainer.\n",
    "    \"\"\"\n",
    "    # Define time step and action specs for one batch.\n",
    "    time_step_spec = trajectories.TimeStep(\n",
    "        step_type=tensor_spec.TensorSpec(\n",
    "            shape=(), dtype=tf.int32, name=\"step_type\"),\n",
    "        reward=tensor_spec.TensorSpec(\n",
    "            shape=(), dtype=tf.float32, name=\"reward\"),\n",
    "        discount=tensor_spec.BoundedTensorSpec(\n",
    "            shape=(), dtype=tf.float32, name=\"discount\", minimum=0.,\n",
    "            maximum=1.),\n",
    "        observation=tensor_spec.TensorSpec(\n",
    "            shape=(rank_k,), dtype=tf.float32,\n",
    "            name=\"observation\"))\n",
    "\n",
    "    action_spec = tensor_spec.BoundedTensorSpec(\n",
    "        shape=(),\n",
    "        dtype=tf.int32,\n",
    "        name=\"action\",\n",
    "        minimum=0,\n",
    "        maximum=num_actions - 1)\n",
    "\n",
    "    # Define RL agent/algorithm.\n",
    "    agent = lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=time_step_spec,\n",
    "        action_spec=action_spec,\n",
    "        tikhonov_weight=tikhonov_weight,\n",
    "        alpha=agent_alpha,\n",
    "        dtype=tf.float32,\n",
    "        accepts_per_arm_features=per_arm)\n",
    "    agent.initialize()\n",
    "    logging.info(\"TimeStep Spec (for each batch):\\n%s\\n\", agent.time_step_spec)\n",
    "    logging.info(\"Action Spec (for each batch):\\n%s\\n\", agent.action_spec)\n",
    "\n",
    "    # Perform off-policy training.\n",
    "    policy, _ = train_policy_on_trajectory(\n",
    "        agent=agent,\n",
    "        tfrecord_file=tfrecord_file,\n",
    "        num_epochs=num_epochs)\n",
    "\n",
    "    # Save trained policy.\n",
    "    logging.info(\"saving policy\")\n",
    "    saver = policy_saver.PolicySaver(policy)\n",
    "    saver.save(training_artifacts_dir)\n",
    "\n",
    "execute_training_and_save_policy(\n",
    "    training_artifacts_dir=training_artifacts_dir,\n",
    "    tfrecord_file=tfrecord_file,\n",
    "    num_epochs=num_epochs,\n",
    "    rank_k=rank_k,\n",
    "    num_actions=num_actions,\n",
    "    tikhonov_weight=tikhonov_weight,\n",
    "    agent_alpha=agent_alpha)\n",
    "\n",
    "outputs = collections.namedtuple(\n",
    "    \"Outputs\",\n",
    "    [\"training_artifacts_dir\"])\n",
    "\n",
    "print(outputs(training_artifacts_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import fastapi\n",
    "\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents import policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIP_STORAGE_URI=\"gs://mlops-vertex-capgemini/artifacts\"\n",
    "tf.saved_model.load(AIP_STORAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate re-training pipeline on streaming data using pubsub and cloud functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator parameters\n",
    "SIMULATOR_PUBSUB_TOPIC = (\n",
    "    \"simulator-pubsub-topic\" \n",
    ")\n",
    "SIMULATOR_CLOUD_FUNCTION = (\n",
    "    \"simulator-cloud-function\"  \n",
    ")\n",
    "SIMULATOR_SCHEDULER_JOB = (\n",
    "    \"simulator-scheduler-job\"  \n",
    ")\n",
    "SIMULATOR_SCHEDULE = \"*/5 * * * *\"  \n",
    "SIMULATOR_SCHEDULER_MESSAGE = (\n",
    "    \"simulator-message\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create $SIMULATOR_PUBSUB_TOPIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_job_args = \" \".join(\n",
    "    [\n",
    "        SIMULATOR_SCHEDULER_JOB,\n",
    "        f\"--schedule='{SIMULATOR_SCHEDULE}'\",\n",
    "        f\"--topic={SIMULATOR_PUBSUB_TOPIC}\",\n",
    "        f\"--message-body={SIMULATOR_SCHEDULER_MESSAGE}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $scheduler_job_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud scheduler jobs create pubsub $scheduler_job_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = ! gcloud beta ai endpoints list \\\n",
    "    --region=$REGION \\\n",
    "    --filter=display_name=$ENDPOINT_DISPLAY_NAME\n",
    "print(\"\\n\".join(endpoints), \"\\n\")\n",
    "\n",
    "ENDPOINT_ID = endpoints[2].split(\" \")[0]\n",
    "print(f\"ENDPOINT_ID={ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "RANK_K=20\n",
    "NUM_ACTIONS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_VARS = \",\".join(\n",
    "    [\n",
    "        f\"PROJECT_ID={PROJECT_ID}\",\n",
    "        f\"REGION={REGION}\",\n",
    "        f\"ENDPOINT_ID={ENDPOINT_ID}\",\n",
    "        f\"RAW_DATA_PATH={RAW_DATA_PATH}\",\n",
    "        f\"BATCH_SIZE={BATCH_SIZE}\",\n",
    "        f\"RANK_K={RANK_K}\",\n",
    "        f\"NUM_ACTIONS={NUM_ACTIONS}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud functions deploy $SIMULATOR_CLOUD_FUNCTION \\\n",
    "    --region=$REGION \\\n",
    "    --trigger-topic=$SIMULATOR_PUBSUB_TOPIC \\\n",
    "    --runtime=python37 \\\n",
    "    --memory=512MB \\\n",
    "    --timeout=200s \\\n",
    "    --source=src/simulator \\\n",
    "    --entry-point=simulate \\\n",
    "    --stage-bucket=$BUCKET_NAME \\\n",
    "    --update-env-vars=$ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create $LOGGER_PUBSUB_TOPIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_VARS = \",\".join(\n",
    "    [\n",
    "        f\"PROJECT_ID={PROJECT_ID}\",\n",
    "        f\"RAW_DATA_PATH={RAW_DATA_PATH}\",\n",
    "        f\"BATCH_SIZE={BATCH_SIZE}\",\n",
    "        f\"RANK_K={RANK_K}\",\n",
    "        f\"NUM_ACTIONS={NUM_ACTIONS}\",\n",
    "        f\"BIGQUERY_TMP_FILE={BIGQUERY_TMP_FILE}\",\n",
    "        f\"BIGQUERY_DATASET_ID={BIGQUERY_DATASET_ID}\",\n",
    "        f\"BIGQUERY_LOCATION={BIGQUERY_LOCATION}\",\n",
    "        f\"BIGQUERY_TABLE_ID={BIGQUERY_TABLE_ID}\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "! echo $ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud functions deploy $LOGGER_CLOUD_FUNCTION \\\n",
    "    --region=$REGION \\\n",
    "    --trigger-topic=$LOGGER_PUBSUB_TOPIC \\\n",
    "    --runtime=python37 \\\n",
    "    --memory=512MB \\\n",
    "    --timeout=200s \\\n",
    "    --source=src/logger \\\n",
    "    --entry-point=log_prediction_to_bigquery \\\n",
    "    --stage-bucket=$BUCKET_NAME \\\n",
    "    --update-env-vars=$ENV_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIGGER_SCHEDULE = \"*/30 * * * *\"  # Schedule to trigger the pipeline. Eg. \"*/30 * * * *\" means every 30 mins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=f\"{PIPELINE_NAME}-retraining\")\n",
    "def pipeline(\n",
    "    # Pipeline configs\n",
    "    project_id: str,\n",
    "    training_artifacts_dir: str,\n",
    "\n",
    "    # BigQuery configs\n",
    "    bigquery_table_id: str,\n",
    "    bigquery_max_rows: int = 10000,\n",
    "\n",
    "    # TF-Agents RL configs\n",
    "    rank_k: int = 20,\n",
    "    num_actions: int = 20,\n",
    "    num_epochs: int = 5,\n",
    "    tikhonov_weight: float = 0.01,\n",
    "    agent_alpha: float = 10) -> None:\n",
    "\n",
    "  # Run the Ingester component.\n",
    "  ingest_op = create_component_from_func(\n",
    "      func=ingester_component.ingest_bigquery_dataset_into_tfrecord,\n",
    "      output_component_file=f\"ingester-{OUTPUT_COMPONENT_SPEC}\",\n",
    "      packages_to_install=[\n",
    "          \"google-cloud-bigquery==2.20.0\",\n",
    "          \"tensorflow==2.5.0\",\n",
    "      ])\n",
    "  ingest_task = ingest_op(\n",
    "      project_id=project_id,\n",
    "      bigquery_table_id=bigquery_table_id,\n",
    "      bigquery_max_rows=bigquery_max_rows,\n",
    "      tfrecord_file=TFRECORD_FILE)\n",
    "\n",
    "  # Run the Trainer component and submit custom job to Vertex AI.\n",
    "  train_op = create_component_from_func(\n",
    "      func=trainer_component.training_op,\n",
    "      output_component_file=f\"trainer-{OUTPUT_COMPONENT_SPEC}\",\n",
    "      packages_to_install=[\n",
    "          \"tensorflow==2.5.0\",\n",
    "          \"tf-agents==0.8.0\",\n",
    "      ])\n",
    "  train_task = train_op(\n",
    "      training_artifacts_dir=training_artifacts_dir,\n",
    "      tfrecord_file=ingest_task.outputs[\"tfrecord_file\"],\n",
    "      num_epochs=num_epochs,\n",
    "      rank_k=rank_k,\n",
    "      num_actions=num_actions,\n",
    "      tikhonov_weight=tikhonov_weight,\n",
    "      agent_alpha=agent_alpha)\n",
    "\n",
    "  worker_pool_specs = [\n",
    "      {\n",
    "          \"containerSpec\": {\n",
    "              \"imageUri\":train_task.container.image,\n",
    "          },\n",
    "          \"replicaCount\": TRAINING_REPLICA_COUNT,\n",
    "          \"machineSpec\": {\n",
    "              \"machineType\": TRAINING_MACHINE_TYPE,\n",
    "              \"acceleratorType\": TRAINING_ACCELERATOR_TYPE,\n",
    "              \"acceleratorCount\": TRAINING_ACCELERATOR_COUNT,\n",
    "          },\n",
    "      },\n",
    "  ]\n",
    "  train_task.custom_job_spec = {\n",
    "      \"displayName\": train_task.name,\n",
    "      \"jobSpec\": {\n",
    "          \"workerPoolSpecs\": worker_pool_specs,\n",
    "      }\n",
    "  }\n",
    "\n",
    "  # Run the Deployer components.\n",
    "  # Upload the trained policy as a model.\n",
    "  model_upload_op = gcc_aip.ModelUploadOp(\n",
    "      project=project_id,\n",
    "      display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "      artifact_uri=training_artifacts_dir,\n",
    "      serving_container_image_uri=f\"gcr.io/{PROJECT_ID}/{PREDICTION_CONTAINER}:latest\",\n",
    "  )\n",
    "  # Model uploading has to occur after training completes.\n",
    "  model_upload_op.after(train_task)\n",
    "  # Create a Vertex AI endpoint. (This operation can occur in parallel with\n",
    "  # the Generator, Ingester, Trainer components.)\n",
    "  endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "      project=project_id,\n",
    "      display_name=ENDPOINT_DISPLAY_NAME)\n",
    "  # Deploy the uploaded, trained policy to the created endpoint. (This operation\n",
    "  # has to occur after both model uploading and endpoint creation complete.)\n",
    "  model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "      project=project_id,\n",
    "      endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "      model=model_upload_op.outputs[\"model\"],\n",
    "      deployed_model_display_name=TRAINED_POLICY_DISPLAY_NAME,\n",
    "      machine_type=ENDPOINT_MACHINE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the authored pipeline.\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,                                                     \n",
    "                            package_path=PIPELINE_SPEC_PATH)\n",
    "\n",
    "# Createa Vertex AI client.\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION)\n",
    "\n",
    "# Schedule a recurring pipeline.\n",
    "response = api_client.create_schedule_from_job_spec(\n",
    "    job_spec_path=PIPELINE_SPEC_PATH,\n",
    "    schedule=TRIGGER_SCHEDULE,\n",
    "    parameter_values={\n",
    "        # Pipeline configs\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"training_artifacts_dir\": TRAINING_ARTIFACTS_DIR,\n",
    "\n",
    "        # BigQuery config\n",
    "        \"bigquery_table_id\": BIGQUERY_TABLE_ID,\n",
    "    })\n",
    "response[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Delete endpoint resource.\n",
    "# ! gcloud ai endpoints delete $ENDPOINT_ID --quiet --region $REGION\n",
    "\n",
    "# # # Delete Pub/Sub topics.\n",
    "# ! gcloud pubsub topics delete $SIMULATOR_PUBSUB_TOPIC --quiet\n",
    "# ! gcloud pubsub topics delete $LOGGER_PUBSUB_TOPIC --quiet\n",
    "\n",
    "# # Delete Cloud Functions.\n",
    "# ! gcloud functions delete $SIMULATOR_CLOUD_FUNCTION --quiet\n",
    "# ! gcloud functions delete $LOGGER_CLOUD_FUNCTION --quiet\n",
    "\n",
    "# # Delete Scheduler job.\n",
    "# ! gcloud scheduler jobs delete $SIMULATOR_SCHEDULER_JOB --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created.\n",
    "# ! gsutil -m rm -r $PIPELINE_ROOT\n",
    "# ! gsutil -m rm -r $TRAINING_ARTIFACTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlops_pipeline_tf_agents_bandits_movie_recommendation.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m86"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
